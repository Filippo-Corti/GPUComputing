{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivxGuz4MVBBQ"
      },
      "source": [
        "---\n",
        "# **LAB 1 - Intro to Numba**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YGa6hXkx77o"
      },
      "source": [
        "# Google Colaboratory (Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-lZoo4TstJn"
      },
      "source": [
        "[Colaboratory](https://research.google.com/colaboratory/faq.html) (or Colab) is a **free research tool** from *Google* for machine learning education and research built on top of [Jupyter Notebook](https://jupyter.org/). It requires no setup and runs entirely in the **cloud**. In Google Colab you can write, execute, save and share your Jupiter Notebooks. You access powerful computing resources like **TPUs** and **GPUs** all for free through your browser. All major Python libraries like **Tensorflow**, **Scikit-learn**, **PyTorch**, **Pandas**, etc. are pre-installed. Google Colab requires no configuration, you only need a **Google Account** and then you are good to go. Your notebooks are stored in your **Google Drive**, or can be loaded from **GitHub**. Colab notebooks can be shared just as you would with Google Docs or Sheets. Simply click the Share button at the top right of any Colab notebook, or follow these Google Drive file sharing instructions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulev4sV3wc-T"
      },
      "source": [
        "### Shell commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS67zRIavQtS"
      },
      "source": [
        "The command `uname` displays the information about the system.\n",
        "\n",
        "* **-a option:** It prints all the system information in the following order: Kernel name, network node hostname,\n",
        "kernel release date, kernel version, machine hardware name, hardware platform, operating system\n",
        "."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT7Umr53u2Qz"
      },
      "outputs": [],
      "source": [
        "!uname -a && cat /etc/*release"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcg1GyK5srek"
      },
      "source": [
        "### Set up Google Drive...\n",
        "\n",
        "That snippet is used in **Google Colab** to mount your **Google Drive**.\n",
        "\n",
        "- Imports the **Colab utility** to access Google Drive\n",
        "- Mounts your Drive at the **path**:\n",
        "`/content/drive/MyDrive/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RDr9PREJBnA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9PmBZql0ow4"
      },
      "source": [
        "# CUDA tools..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTRXba1Msgq2"
      },
      "source": [
        "**NVIDIA System Management Interface (nvidia-smi)**\n",
        "\n",
        "The NVIDIA System Management Interface (**`nvidia-smi`**) is a command line utility, based on top of the NVIDIA Management Library (NVML), intended to aid in the **management** and **monitoring** of NVIDIA GPU devices.\n",
        "\n",
        "This utility allows administrators to query GPU device state and with the appropriate privileges, permits administrators to modify GPU device state.  It is targeted at the TeslaTM, GRIDTM, QuadroTM and Titan X product, though limited support is also available on other NVIDIA GPUs.\n",
        "\n",
        "For more details, please refer to the **`nvidia-smi`** documentation ([doc](http://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf))\n",
        "\n",
        "For information on **Tesla T4** see:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlXMCnBVBkeE"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skebag2Wd0al"
      },
      "source": [
        "**Numba code used as a CUDA sanity check**:\n",
        "-   Imports Numba, a JIT compiler that accelerates Python code.\n",
        "-\tnumba.cuda provides GPU (CUDA) support using NVIDIA GPUs.\n",
        "\t- âœ” Confirms NumPy and Numba are installed\n",
        "\t- âœ” Confirms CUDA drivers are visible\n",
        "\t- âœ” Confirms GPU compute capability\n",
        "\t- âœ” Helps debug environment issues before running GPU kernels\n",
        "\n",
        "Probes the system for available CUDA-capable GPUs. Prints:\n",
        "-\tNumber of GPUs\n",
        "-\tGPU names\n",
        "-\tCompute capability\n",
        "-\tDriver/runtime status\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMGhyLJjd0am"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numba\n",
        "from numba import cuda\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(np.__version__)\n",
        "print(numba.__version__)\n",
        "\n",
        "cuda.detect()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB94Ce6kViOj"
      },
      "source": [
        "# âœ… Hello World!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIlUaQYgdooS"
      },
      "source": [
        "**My first CUDA program: HelloFromGPU!**\n",
        "\n",
        "CUDA kernel for Hello world..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBQJxfT4d0an"
      },
      "outputs": [],
      "source": [
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def hello_kernel():\n",
        "    print(\"Hello from GPU!\") # This is not printing in Colab (but it should...)\n",
        "\n",
        "print(\"Starting...\")\n",
        "hello_kernel[1, 10]() # 1 Block, 10 Threads\n",
        "cuda.synchronize()\n",
        "print(\"...Synchronized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VJXO5U8JSGz"
      },
      "source": [
        "# âœ… Parallel vector sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EK4vVO-ud0ao"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def add_arrays_cuda(a, b, c):\n",
        "    i = cuda.grid(1) # Return the absolute position of the current thread in the entire grid of blocks (1 means to interpret grid as 1D).\n",
        "    assert i < c.size\n",
        "    c[i] = a[i] + b[i]\n",
        "\n",
        "# Example\n",
        "n = 1000    # size of arrays = number of threads\n",
        "a = np.ones(n, dtype=np.float32) # Notice float32! (Default for GPUs - but not for Python)\n",
        "b = np.ones(n, dtype=np.float32)\n",
        "\n",
        "d_a = cuda.to_device(a) # Sends to Device (the GPU)\n",
        "d_b = cuda.to_device(b)\n",
        "d_c = cuda.device_array_like(a) # Allocates an array directy on GPU\n",
        "\n",
        "# Start the n Threads. then wait for all of them to finish\n",
        "add_arrays_cuda[1, n](d_a, d_b, d_c)\n",
        "cuda.synchronize()\n",
        "\n",
        "c = d_c.copy_to_host() # Copies to Host (the CPU)\n",
        "print(c[:10], c[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku--dgMncvGP"
      },
      "source": [
        "## â†˜ï¸ TODO..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6sdTNp8d0ao"
      },
      "source": [
        "**Check Fibonacci Membership**\n",
        "\n",
        "ðŸ”¹ **Exercise Goal**\n",
        "\n",
        "-   Given a vector $x$ of integers, build a CUDA kernel (Numba) that produces a vector $y$ such that:\n",
        "\n",
        "$$\n",
        "y_i =\n",
        "\\begin{cases}\n",
        "1 & \\text{if } x_i \\text{ is a Fibonacci number} \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "ðŸ”¹ **Input**\n",
        "\n",
        "- `x`: 1D array of integers (e.g., `np.int32`)\n",
        "- `n = x.size`\n",
        "\n",
        "ðŸ”¹ **Output**\n",
        "\n",
        "- `y`: 1D array of `np.uint8` or `np.int32`\n",
        "- Same length as `x`\n",
        "\n",
        "\n",
        "ðŸ”¹ **Constraints**\n",
        "\n",
        "- One GPU thread handles one element:\n",
        "  $$\n",
        "  i = \\text{cuda.grid}(1)\n",
        "  $$\n",
        "- Must include a bounds check:\n",
        "  $$\n",
        "  i < n\n",
        "  $$\n",
        "- Avoid Python objects and dynamic allocation inside the kernel\n",
        "\n",
        "\n",
        "ðŸ”¹ **A classic property:**\n",
        "\n",
        "> An integer $v \\ge 0$ is Fibonacci **iff** one of these is a perfect square:\n",
        "$$\n",
        "5v^2 + 4 \\quad \\text{or} \\quad 5v^2 - 4\n",
        "$$\n",
        "\n",
        "\n",
        "ðŸ”¹ **Your Tasks**\n",
        "\n",
        "1. Write a device function:\n",
        "   - `is_perfect_square(m) -> bool`\n",
        "\n",
        "2. Write a device function:\n",
        "   - `is_fib(v) -> bool`\n",
        "\n",
        "3. Write a kernel:\n",
        "   - `fib_mask(x, y)` that sets `y[i] = 1` if `x[i]` is Fibonacci else `0`\n",
        "\n",
        "4. Write host code to:\n",
        "   - allocate/copy arrays to GPU\n",
        "   - launch the kernel\n",
        "   - copy results back and validate\n",
        "\n",
        "ðŸ”¹ **Expected result**\n",
        "-    for the first $n = 1000$ integers: [0,   1,   2,   3,   5,   8,  13,  21,  34,  55,  89, 144, 233, 377, 610, 987]\n",
        "\n",
        "ðŸ”¹ **Skeleton Code (Fill the TODOs)**\n",
        "\n",
        "```{python}\n",
        "    #| echo: true\n",
        "    import numpy as np\n",
        "    from numba import cuda\n",
        "    import math\n",
        "\n",
        "    @cuda.jit\n",
        "    def fib_numbers(x,y):\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "    # input data\n",
        "        # TODO\n",
        "\n",
        "    # GPU memory allocation\n",
        "        # TODO\n",
        "\n",
        "    # Kernel launch\n",
        "        # TODO\n",
        "\n",
        "    # Copy back results & print indexes of Fibonacci numbers\n",
        "        # TODO\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVajilaYjft5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "import math\n",
        "\n",
        "@cuda.jit\n",
        "def is_perfect_square(m: int) -> bool:\n",
        "  \"\"\"Returns true if m is a perfect square\"\"\"\n",
        "  sqr = math.floor(m ** 0.5)\n",
        "  return sqr * sqr == m\n",
        "\n",
        "@cuda.jit\n",
        "def is_fib(v: int) -> bool:\n",
        "  \"\"\"Returns true if v is a Fibonacci number\"\"\"\n",
        "  return is_perfect_square(5 * v * v + 4) or is_perfect_square(5 * v * v - 4)\n",
        "\n",
        "@cuda.jit\n",
        "def fib_mask(x: np.ndarray, y: np.ndarray):\n",
        "  \"\"\"Sets y[i] to 1 if x[i] is a Fibonacci number\"\"\"\n",
        "  i = cuda.grid(1)\n",
        "  if i < x.size and is_fib(x[i]): # Remember to check size\n",
        "    y[i] = 1\n",
        "  else:\n",
        "    y[i] = 0\n",
        "\n",
        "\n",
        "# input data\n",
        "n = 1000\n",
        "x = np.arange(0, n, dtype=np.int32)\n",
        "y = np.zeros(n, dtype=np.uint8) # I could also create this directly on Device using cuda.device_array\n",
        "\n",
        "# GPU memory allocation\n",
        "d_x = cuda.to_device(x)\n",
        "d_y = cuda.to_device(y)\n",
        "\n",
        "# Kernel launch\n",
        "fib_mask[1, n](d_x, d_y)\n",
        "\n",
        "# Copy back results & print indexes of Fibonacci numbers\n",
        "cuda.synchronize()\n",
        "y = d_y.copy_to_host()\n",
        "for num, is_fib in zip(x, y):\n",
        "  if is_fib == 1:\n",
        "    print(num, end=\" \")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmRSZhvtmMUa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAI7ZfNQmYd5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9YGa6hXkx77o",
        "F9PmBZql0ow4",
        "227eLdP5csN1",
        "4bll8pe7Fj3y"
      ],
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}